---
title: "Conv Nets"
output: html_notebook
---

In the first example, we "stretched" the matrix of pixels into a long vector which served as the input of the feedforward neural net. In practice, most image processing applications utilize another type of architecture known as convolutional neural networks (CNN), which exploits the spatial relationship among the pixels.

We'll continue with the `mnist` dataset from the previous section.

```{r}
library(keras)
mnist <- dataset_mnist()
str(mnist)
```

Note the call to `array_reshape()` below --- we're reshaping the images to volumes of dimension $img\_rows \times img\_cols \times 1$. The last dimension, *depth*, in this case is `1` since our images are grayscale. 

```{r}
img_rows <- 28
img_cols <- 28

c(c(x_train, y_train), c(x_test, y_test)) %<-% mnist

x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)

# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255

# Convert class vectors to binary class matrices
num_classes <- 10
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)

dim(x_train)
```

We're now ready to define a neural network model!

```{r}
# Instantiate a Keras sequential model object
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), 
                activation = 'relu', input_shape = input_shape) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), 
                strides = 2, activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = num_classes, activation = 'softmax')

summary(model)
```

Now, we can compile the model!

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = "adam",
  metrics = c('accuracy')
)
```

Now that we have a compiled model, we're ready to start training!

```{r}
batch_size <- 128
epochs <- 10

history <- model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 1,
  validation_split = 0.2
)

plot(history)
```

Once the model is trained, we can `evaluate` it and look at some performance metrics

```{r}
score <- model %>% evaluate(
  x_test, y_test,
  verbose = 0
)

# Output metrics
cat('Test loss:', score[[1]], '\n')
cat('Test accuracy:', score[[2]], '\n')
```

